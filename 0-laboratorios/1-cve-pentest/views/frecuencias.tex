\section{Criptoanálisis de Sustitución Monoalfabética}

\begin{cryptoanalysis}{Fundamentos del Criptoanálisis de Frecuencias}
El criptoanálisis de frecuencias es una técnica fundamental para romper cifrados de sustitución monoalfabética. Se basa en un principio lingüístico simple pero poderoso: en cada idioma, ciertas letras y combinaciones de letras aparecen con frecuencias predecibles y consistentes.

Por ejemplo, en español:
\begin{itemize}
    \item Las letras más frecuentes son E (13.68\%), A (12.53\%), O (8.68\%) y S (7.98\%)
    \item Las letras menos frecuentes son K (0.00\%), W (0.02\%), X (0.22\%) y Z (0.52\%)
    \item Los bigramas más comunes incluyen ES, EN, EL, DE, LA
    \item Los trigramas más comunes incluyen QUE, EST, DEL, LOS, LAS
\end{itemize}

Estas regularidades estadísticas constituyen una "huella digital" del idioma que persiste incluso cuando el texto ha sido cifrado mediante sustitución simple.
\end{cryptoanalysis}

\subsection{Metodología de Ataque}

\begin{cryptomethod}{Proceso de Descifrado Estadístico}
El enfoque sistemático para romper un cifrado monoalfabético consta de las siguientes etapas:

\begin{enumerate}
    \item \textbf{Análisis de frecuencia de caracteres}: Se contabiliza la frecuencia de aparición de cada símbolo en el texto cifrado.
    
    \item \textbf{Mapeo inicial por frecuencias}: Se establece una correspondencia tentativa entre los símbolos cifrados y las letras del alfabeto, basándose en la distribución de frecuencias del idioma objetivo.
    
    \item \textbf{Identificación de patrones}: Se buscan patrones que podrían corresponder a palabras cortas comunes (artículos, preposiciones, conjunciones).
    
    \item \textbf{Refinamiento del mapeo}: Se ajusta el mapeo inicial utilizando el conocimiento de la estructura léxica del idioma.
    
    \item \textbf{Optimización mediante algoritmos heurísticos}: Se aplican técnicas como hill climbing o recocido simulado para mejorar iterativamente la solución.
    
    \item \textbf{Validación lingüística}: Se evalúa la coherencia del texto descifrado mediante el análisis de n-gramas y la verificación contra diccionarios.
\end{enumerate}

El proceso no es puramente secuencial, sino iterativo, con retroalimentación constante entre las diferentes etapas.
\end{cryptomethod}

\subsection{Implementación Algorítmica}

La implementación de un decodificador automático para cifrados monoalfabéticos requiere combinar técnicas estadísticas con heurísticas de optimización. A continuación se describe cada componente clave:

\begin{securitygoodpractice}
\textbf{Análisis de Frecuencias}

El primer paso consiste en analizar la distribución estadística de los símbolos en el texto cifrado:

\begin{enumerate}
    \item Se filtran los caracteres relevantes del texto cifrado
    \item Se cuenta la frecuencia de cada símbolo
    \item Se convierten los conteos a porcentajes
    \item Se ordenan los símbolos por frecuencia descendente
\end{enumerate}

Esto proporciona una primera aproximación a la posible correspondencia entre símbolos cifrados y letras del alfabeto español.
\end{securitygoodpractice}

\begin{cryptoanalysis}{Mapeo Inicial y Patrones}
El mapeo inicial se crea comparando la distribución de frecuencias del texto cifrado con la distribución conocida del español:

$$\text{mapeo\_inicial} = \{s_1 \rightarrow l_1, s_2 \rightarrow l_2, \ldots, s_n \rightarrow l_n\}$$

donde $s_i$ es el $i$-ésimo símbolo más frecuente en el texto cifrado y $l_i$ es la $i$-ésima letra más frecuente en español.

Este mapeo se refina identificando patrones que podrían corresponder a palabras comunes. Por ejemplo, si encontramos que una secuencia de símbolos como "XX" aparece frecuentemente en posiciones donde esperaríamos palabras como "DE", "EL" o "LA", podemos ajustar el mapeo para que refleje esta observación.
\end{cryptoanalysis}

\begin{securityalert}
\textbf{Desafíos del Mapeo Inicial}

El mapeo basado únicamente en frecuencias individuales suele ser insuficiente porque:

\begin{itemize}
    \item Textos cortos pueden tener distribuciones de frecuencia atípicas
    \item Textos especializados pueden desviarse de la distribución promedio del idioma
    \item La distribución exacta varía según el estilo, período y tema del texto
\end{itemize}

Por esto, es crucial complementar el análisis de frecuencias con técnicas de optimización que exploren sistemáticamente el espacio de posibles soluciones.
\end{securityalert}

\subsection{Optimización mediante Hill Climbing}

\begin{cryptomethod}{Algoritmo de Hill Climbing con Simulated Annealing}
Para refinar el mapeo inicial, se implementa un algoritmo de hill climbing mejorado con simulated annealing:

\begin{align}
\text{Coherencia}(M) &= f(\text{bigramas}, \text{trigramas}, \text{patrones\_léxicos}) \\
\Delta &= \text{Coherencia}(M_{\text{nuevo}}) - \text{Coherencia}(M_{\text{actual}}) \\
P(\text{aceptar}) &= 
\begin{cases}
1 & \text{si } \Delta > 0 \\
e^{\Delta/T} & \text{si } \Delta \leq 0
\end{cases}
\end{align}

donde:
\begin{itemize}
    \item $M$ representa un mapeo entre símbolos cifrados y letras
    \item $T$ es la temperatura, que disminuye gradualmente a lo largo de las iteraciones
    \item $f$ es una función que evalúa la coherencia lingüística del texto descifrado
\end{itemize}

El algoritmo:
\begin{enumerate}
    \item Comienza con el mapeo inicial basado en frecuencias
    \item En cada iteración, selecciona aleatoriamente dos símbolos y intercambia sus mapeos
    \item Evalúa la coherencia del texto descifrado con el nuevo mapeo
    \item Acepta el cambio si mejora la coherencia o, con cierta probabilidad, incluso si la empeora (para escapar de óptimos locales)
    \item Disminuye gradualmente la temperatura, reduciendo la probabilidad de aceptar cambios que empeoren la solución
    \item Continúa hasta alcanzar un criterio de parada (número máximo de iteraciones o umbral de coherencia)
\end{enumerate}
\end{cryptomethod}

\subsection{Evaluación de Coherencia}

\begin{cryptoanalysis}{Métricas de Coherencia Lingüística}
La clave del éxito en el algoritmo de optimización es la función que evalúa la coherencia del texto descifrado. Esta función combina varias métricas:

\begin{align}
\text{Coherencia} = w_1 \cdot \frac{|\text{Bigramas}_{\text{encontrados}}|}{|\text{Bigramas}_{\text{esperados}}|} + 
w_2 \cdot \frac{|\text{Trigramas}_{\text{encontrados}}|}{|\text{Trigramas}_{\text{esperados}}|} + 
w_3 \cdot \text{FreqScore} + 
w_4 \cdot \text{DictScore}
\end{align}

donde:
\begin{itemize}
    \item $\text{Bigramas}_{\text{encontrados}}$ y $\text{Trigramas}_{\text{encontrados}}$ son los n-gramas del texto descifrado que coinciden con n-gramas comunes en español.
    \item $\text{FreqScore}$ evalúa si los n-gramas más frecuentes en el texto descifrado son también frecuentes en español.
    \item $\text{DictScore}$ mide el porcentaje de palabras descifradas que aparecen en un diccionario español.
    \item $w_1, w_2, w_3, w_4$ son pesos que determinan la importancia relativa de cada componente.
\end{itemize}

Esta función multicritério permite capturar diferentes aspectos de la coherencia lingüística, aumentando la robustez del algoritmo.
\end{cryptoanalysis}

\subsection{Resultados Experimentales}

\begin{securitygoodpractice}
\textbf{Caso de Estudio: Descifrado de un Texto Cifrado}

Al aplicar este enfoque a un texto cifrado mediante sustitución monoalfabética, se logró obtener un descifrado con un 86.90\% de coherencia:

\begin{quote}
ACTUALMENTE DISPONEMOS DE UNA GRAN CANTIDAD DE NAYEGADORES FEK PARA ELEGIR EL NUESTRO CONCEPTOS COMO LA SEGURIDAD O EL...
\end{quote}

El texto resultante es mayoritariamente legible, con solo algunos errores (como "NAYEGADORES FEK" en lugar de "NAVEGADORES WEB") que podrían corregirse manualmente o mediante un refinamiento adicional del algoritmo.

El tiempo de ejecución fue de aproximadamente 2 minutos con 20,000 iteraciones del algoritmo de optimización, lo que demuestra la eficiencia del enfoque incluso para textos de longitud considerable.
\end{securitygoodpractice}

\subsection{Consideraciones y Limitaciones}

\begin{securitywarning}
\textbf{Factores que Afectan el Rendimiento del Descifrado}

El rendimiento del algoritmo puede verse afectado por diversos factores:

\begin{itemize}
    \item \textbf{Longitud del texto}: Textos muy cortos pueden no proporcionar suficientes estadísticas para un descifrado confiable.
    \item \textbf{Caracteres especiales}: La presencia de símbolos, números o signos de puntuación puede complicar el análisis.
    \item \textbf{Modificaciones al cifrado básico}: Variantes como cifrados polialfabéticos o homofónicos requieren enfoques más sofisticados.
    \item \textbf{Especificidad del contenido}: Textos con terminología técnica o distribuciones atípicas de letras pueden resultar más difíciles de descifrar.
\end{itemize}

Para superar estas limitaciones, se pueden implementar estrategias como el análisis contextual, la incorporación de conocimiento del dominio y el refinamiento manual interactivo.
\end{securitywarning}

\section{Conclusiones}

La implementación de un decodificador automático para cifrados monoalfabéticos demuestra cómo los principios fundamentales del criptoanálisis de frecuencias, combinados con técnicas modernas de optimización combinatoria, pueden romper eficientemente esquemas de cifrado que alguna vez se consideraron seguros.

El éxito del enfoque descrito, con una coherencia del 86.90\%, ilustra por qué los cifrados de sustitución simple ya no se utilizan en aplicaciones de seguridad modernas. Al mismo tiempo, el estudio de estas técnicas proporciona una base sólida para comprender métodos criptográficos más avanzados y sus vulnerabilidades potenciales.

Este caso práctico refuerza un principio fundamental de la seguridad informática: la importancia de evaluar los sistemas criptográficos no solo desde la perspectiva de su diseño teórico, sino también considerando su resistencia a técnicas de análisis establecidas y emergentes.