\section{Criptoanálisis de Sustitución Monoalfabética}

\begin{cryptoanalysis}{Fundamentos del Criptoanálisis de Frecuencias}
El criptoanálisis de frecuencias es una técnica fundamental para romper cifrados de sustitución monoalfabética. Se basa en un principio lingüístico simple pero poderoso: en cada idioma, ciertas letras y combinaciones de letras aparecen con frecuencias predecibles y consistentes.

Por ejemplo, en español:
\begin{itemize}
    \item Las letras más frecuentes son E (13.68\%), A (12.53\%), O (8.68\%) y S (7.98\%)
    \item Las letras menos frecuentes son K (0.00\%), W (0.02\%), X (0.22\%) y Z (0.52\%)
    \item Los bigramas más comunes incluyen ES, EN, EL, DE, LA
    \item Los trigramas más comunes incluyen QUE, EST, DEL, LOS, LAS
\end{itemize}

Estas regularidades estadísticas constituyen una "huella digital" del idioma que persiste incluso cuando el texto ha sido cifrado mediante sustitución simple.
\end{cryptoanalysis}

\subsection{Metodología de Ataque}

\begin{cryptomethod}{Proceso de Descifrado Estadístico}
El enfoque sistemático para romper un cifrado monoalfabético consta de las siguientes etapas:

\begin{enumerate}
    \item \textbf{Análisis de frecuencia de caracteres}: Se contabiliza la frecuencia de aparición de cada símbolo en el texto cifrado.
    
    \item \textbf{Mapeo inicial por frecuencias}: Se establece una correspondencia tentativa entre los símbolos cifrados y las letras del alfabeto, basándose en la distribución de frecuencias del idioma objetivo.
    
    \item \textbf{Identificación de patrones}: Se buscan patrones que podrían corresponder a palabras cortas comunes (artículos, preposiciones, conjunciones).
    
    \item \textbf{Refinamiento del mapeo}: Se ajusta el mapeo inicial utilizando el conocimiento de la estructura léxica del idioma.
    
    \item \textbf{Optimización mediante algoritmos heurísticos}: Se aplican técnicas como hill climbing o recocido simulado para mejorar iterativamente la solución.
    
    \item \textbf{Validación lingüística}: Se evalúa la coherencia del texto descifrado mediante el análisis de n-gramas y la verificación contra diccionarios.
\end{enumerate}

El proceso no es puramente secuencial, sino iterativo, con retroalimentación constante entre las diferentes etapas.
\end{cryptomethod}

\subsection{Implementación Algorítmica}

La implementación de un decodificador automático para cifrados monoalfabéticos requiere combinar técnicas estadísticas con heurísticas de optimización. A continuación se describe cada componente clave:

\begin{securitygoodpractice}
\textbf{Análisis de Frecuencias}

El primer paso consiste en analizar la distribución estadística de los símbolos en el texto cifrado:

\begin{enumerate}
    \item Se filtran los caracteres relevantes del texto cifrado
    \item Se cuenta la frecuencia de cada símbolo
    \item Se convierten los conteos a porcentajes
    \item Se ordenan los símbolos por frecuencia descendente
\end{enumerate}

Esto proporciona una primera aproximación a la posible correspondencia entre símbolos cifrados y letras del alfabeto español.
\end{securitygoodpractice}

\begin{cryptoanalysis}{Mapeo Inicial y Patrones}
El mapeo inicial se crea comparando la distribución de frecuencias del texto cifrado con la distribución conocida del español:

$$\text{mapeo\_inicial} = \{s_1 \rightarrow l_1, s_2 \rightarrow l_2, \ldots, s_n \rightarrow l_n\}$$

donde $s_i$ es el $i$-ésimo símbolo más frecuente en el texto cifrado y $l_i$ es la $i$-ésima letra más frecuente en español.

Este mapeo se refina identificando patrones que podrían corresponder a palabras comunes. Por ejemplo, si encontramos que una secuencia de símbolos como "XX" aparece frecuentemente en posiciones donde esperaríamos palabras como "DE", "EL" o "LA", podemos ajustar el mapeo para que refleje esta observación.
\end{cryptoanalysis}

\begin{securityalert}
\textbf{Desafíos del Mapeo Inicial}

El mapeo basado únicamente en frecuencias individuales suele ser insuficiente porque:

\begin{itemize}
    \item Textos cortos pueden tener distribuciones de frecuencia atípicas
    \item Textos especializados pueden desviarse de la distribución promedio del idioma
    \item La distribución exacta varía según el estilo, período y tema del texto
\end{itemize}

Por esto, es crucial complementar el análisis de frecuencias con técnicas de optimización que exploren sistemáticamente el espacio de posibles soluciones.
\end{securityalert}

\subsection{Optimización mediante Hill Climbing}

\begin{cryptomethod}{Algoritmo de Hill Climbing con Simulated Annealing}
Para refinar el mapeo inicial, se implementa un algoritmo de hill climbing mejorado con simulated annealing:

\begin{align}
\text{Coherencia}(M) &= f(\text{bigramas}, \text{trigramas}, \text{patrones\_léxicos}) \\
\Delta &= \text{Coherencia}(M_{\text{nuevo}}) - \text{Coherencia}(M_{\text{actual}}) \\
P(\text{aceptar}) &= 
\begin{cases}
1 & \text{si } \Delta > 0 \\
e^{\Delta/T} & \text{si } \Delta \leq 0
\end{cases}
\end{align}

donde:
\begin{itemize}
    \item $M$ representa un mapeo entre símbolos cifrados y letras
    \item $T$ es la temperatura, que disminuye gradualmente a lo largo de las iteraciones
    \item $f$ es una función que evalúa la coherencia lingüística del texto descifrado
\end{itemize}

El algoritmo:
\begin{enumerate}
    \item Comienza con el mapeo inicial basado en frecuencias
    \item En cada iteración, selecciona aleatoriamente dos símbolos y intercambia sus mapeos
    \item Evalúa la coherencia del texto descifrado con el nuevo mapeo
    \item Acepta el cambio si mejora la coherencia o, con cierta probabilidad, incluso si la empeora (para escapar de óptimos locales)
    \item Disminuye gradualmente la temperatura, reduciendo la probabilidad de aceptar cambios que empeoren la solución
    \item Continúa hasta alcanzar un criterio de parada (número máximo de iteraciones o umbral de coherencia)
\end{enumerate}
\end{cryptomethod}

\subsection{Evaluación de Coherencia}

\begin{cryptoanalysis}{Métricas de Coherencia Lingüística}
La clave del éxito en el algoritmo de optimización es la función que evalúa la coherencia del texto descifrado. Esta función combina varias métricas:

\begin{align}
\text{Coherencia} = w_1 \cdot \frac{|\text{Bigramas}_{\text{encontrados}}|}{|\text{Bigramas}_{\text{esperados}}|} + 
w_2 \cdot \frac{|\text{Trigramas}_{\text{encontrados}}|}{|\text{Trigramas}_{\text{esperados}}|} + \\
w_3 \cdot \text{FreqScore} + 
w_4 \cdot \text{DictScore}
\end{align}

donde:
\begin{itemize}
    \item $\text{Bigramas}_{\text{encontrados}}$ y $\text{Trigramas}_{\text{encontrados}}$ son los n-gramas del texto descifrado que coinciden con n-gramas comunes en español.
    \item $\text{FreqScore}$ evalúa si los n-gramas más frecuentes en el texto descifrado son también frecuentes en español.
    \item $\text{DictScore}$ mide el porcentaje de palabras descifradas que aparecen en un diccionario español.
    \item $w_1, w_2, w_3, w_4$ son pesos que determinan la importancia relativa de cada componente.
\end{itemize}

Esta función multicriterio permite capturar diferentes aspectos de la coherencia lingüística, aumentando la robustez del algoritmo.
\end{cryptoanalysis}

\subsection{Resultados Experimentales}

\begin{securitygoodpractice}
\textbf{Caso de Estudio: Descifrado de un Texto sobre Navegadores Web}

Al aplicar nuestro enfoque automático a un texto cifrado mediante sustitución monoalfabética, se logró obtener un descifrado con un 86.90\% de coherencia. El texto resultante trataba sobre navegadores web y su rendimiento en diferentes sistemas operativos:

\begin{quote}
ACTUALMENTE DISPONEMOS DE UNA GRAN CANTIDAD DE NAVEGADORES WEB PARA ELEGIR EL NUESTRO CONCEPTOS COMO LA SEGURIDAD O EL CUMPLIMIENTO DE LOS ESTÁNDARES DEL WC WORLD WIDE WEB CONSORTIUM...
\end{quote}

El algoritmo identificó correctamente la mayoría de las sustituciones, como se puede observar en el mapeo parcial obtenido:
\begin{verbatim}
| ! -> S | # -> U | % -> J | ( -> G | ) -> R | * -> C | @ -> P |
| ^ -> T | a -> A | b -> I | c -> v | d -> f | f -> X | g -> h |
| h -> M | i -> b | j -> y | k -> E | l -> Q | m -> O | o -> x |
| p -> N | q -> H | r -> w | s -> L | t -> D |
\end{verbatim}

Sin embargo, algunas sustituciones requirieron corrección manual posterior, particularmente en letras menos frecuentes o con contextos ambiguos:
\begin{verbatim}
| Y -> V | F -> W | K -> B | B -> X | Z -> Y | W -> H | V -> F |
\end{verbatim}

Esta fase de refinamiento manual es común en el criptoanálisis práctico, donde el conocimiento lingüístico humano complementa el análisis algorítmico para resolver ambigüedades y mejorar la calidad del descifrado.
\end{securitygoodpractice}

\begin{cryptoanalysis}{Análisis del Contenido Descifrado}
El texto descifrado resultó ser un artículo comparativo sobre la velocidad de diferentes navegadores web en distintos sistemas operativos:

\begin{itemize}
    \item Opera, un navegador noruego, se identificó como el más rápido en la mayoría de las pruebas.
    \item Firefox, que había reclamado ser el más rápido, mostró resultados inconsistentes.
    \item Internet Explorer "aguantó bien", contradiciendo a sus críticos.
    \item Se analizaron resultados en Windows, Linux (distribución SUSE) y Mac OS X.
    \item Para Mac, Safari (de Apple) y Camino (del proyecto Mozilla) también obtuvieron buenos resultados en algunas categorías.
\end{itemize}

Este contenido técnico, con términos específicos como "renderización", "CSS" y nombres de productos, representó un desafío adicional para el algoritmo de descifrado debido a la presencia de palabras que no siguen los patrones típicos del español general.
\end{cryptoanalysis}

\subsection{Consideraciones y Limitaciones}

\begin{securitywarning}
\textbf{Factores que Afectan el Rendimiento del Descifrado}

El rendimiento del algoritmo puede verse afectado por diversos factores:

\begin{itemize}
    \item \textbf{Terminología especializada}: Palabras técnicas como "browser", "renderización" o nombres propios como "Opera" y "Mozilla" pueden complicar el análisis basado en frecuencias generales del español.
    
    \item \textbf{Ambigüedad en letras poco frecuentes}: Letras como W, X, Y, Z son particularmente difíciles de mapear correctamente mediante análisis automatizado debido a su baja frecuencia en español.
    
    \item \textbf{Ausencia de contexto semántico}: El algoritmo no comprende el significado del texto, lo que limita su capacidad para resolver ambigüedades que un humano podría detectar fácilmente por contexto.
    
    \item \textbf{Variaciones ortográficas}: El texto contiene términos en inglés y nombres propios que no siguen los patrones ortográficos del español.
\end{itemize}

Estos factores explican por qué fue necesario el refinamiento manual de algunas sustituciones para alcanzar un texto completamente legible.
\end{securitywarning}

\section{Conclusiones}

\begin{cryptomethod}{El Valor del Análisis Combinado}
El caso práctico presentado demuestra la efectividad de combinar enfoques algorítmicos y humanos en el criptoanálisis:

\begin{enumerate}
    \item \textbf{Automatización eficiente}: El algoritmo logró descifrar aproximadamente el 87\% del texto de forma autónoma, reduciendo drásticamente el tiempo y esfuerzo requeridos.
    
    \item \textbf{Intervención humana estratégica}: La corrección manual de un pequeño subconjunto de sustituciones completó el descifrado, aprovechando la comprensión contextual que los algoritmos actuales aún no poseen.
    
    \item \textbf{Enfoque práctico}: Este método híbrido representa el enfoque real utilizado por los criptoanalistas: herramientas automatizadas para el trabajo pesado y análisis humano para las decisiones finales.
\end{enumerate}

Este exitoso descifrado ilustra por qué los cifrados de sustitución monoalfabética, aunque históricamente importantes, son considerados inseguros en la criptografía moderna. A diferencia de algoritmos como DES, que requieren enfoques computacionalmente intensivos para ser quebrados, los cifrados monoalfabéticos sucumben ante análisis estadísticos relativamente sencillos combinados con optimización heurística.
\end{cryptomethod}